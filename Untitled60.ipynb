{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593ba60-cbc4-4484-a1b7-5731e74bbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Polynomial functions can be used as kernel functions in machine learning algorithms, including Support Vector Machines (SVMs). \n",
    "Kernel functions are used to transform the input data into a higher-dimensional feature space, making it easier to separate data that is not linearly separable in the original feature space. \n",
    "Polynomial kernel functions are one type of kernel function that can be used to achieve this transformation. They take the form of (gamma * (x * x') + coef0)^degree, where x and x' are data points,\n",
    "gamma controls the shape of the decision boundary, coef0 is a constant, and degree is the degree of the polynomial. By adjusting these parameters, you can control the complexity of the decision boundary.\n",
    "\n",
    "2. To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can use the `SVC` (Support Vector Classification) class with the `kernel` parameter set to 'poly'. Here's a code snippet:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)  # You can adjust degree and C as needed\n",
    "\n",
    "# Train the SVM on your data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "```\n",
    "\n",
    "3. Increasing the value of epsilon in Support Vector Regression (SVR) tends to increase the number of support vector \n",
    "   Epsilon, denoted as ε, controls the width of the epsilon-tube around the predicted values within which no penalty is associated with errors. When ε is larger, a larger margin is allowed, which means that more data points can fall within the margin without incurring a penalty, leading to a larger number of support vectors.\n",
    "\n",
    "4. The performance of Support Vector Regression (SVR) is affected by the following parameters:\n",
    "\n",
    "   - Kernel Function: The choice of kernel function, such as linear, polynomial, or radial basis function (RBF), can significantly impact the performance.\n",
    "    The appropriate kernel depends on the problem's characteristics. For instance, RBF kernels work well for complex, non-linear relationships.\n",
    "\n",
    "   - C Parameter: The C parameter controls the trade-off between fitting the training data and allowing errors. Higher values of C prioritize fitting the training data closely, which can lead to overfitting, while lower values allow for more tolerance to errors and can lead to underfitting.\n",
    "\n",
    "   - Epsilon (ε) Parameter: As mentioned earlier, epsilon controls the width of the epsilon-tube. A smaller epsilon encourages a narrower tube and may result in fewer support vectors and a more rigid fit, while a larger epsilon allows more data points to fall within the margin.\n",
    "\n",
    "   - Gamma Parameter: In RBF and polynomial kernels, gamma controls the shape of the decision boundary. Smaller values of gamma result in a smoother decision boundary, while larger values lead to a more complex and wiggly boundary.\n",
    "\n",
    "   Adjusting these parameters should be done through hyperparameter tuning and cross-validation to find the values that optimize the SVR model's performance for a specific dataset and problem. It's often necessary to experiment and iterate to determine the best parameter settings for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cf208a-45e2-401f-9051-8cfaad813e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import the necessary libraries and load the dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib  # To save the trained classifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it on the training data\n",
    "svc_classifier = SVC(kernel='rbf', C=1.0)  # You can change the kernel and C value\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Step 7: Tune hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': [0.1, 1, 'scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and estimator from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_svc_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "best_svc_classifier.fit(X, y)\n",
    "\n",
    "# Step 9: Save the trained classifier to a file for future use\n",
    "joblib.dump(best_svc_classifier, 'svm_classifier.pkl')\n",
    "\n",
    "# You can load the classifier later using joblib.load('svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f9c15-553b-40fc-993b-b8b27f8b8018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
